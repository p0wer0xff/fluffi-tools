{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "import experiment\n",
    "import extract\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data\n",
    "df_measurements = pd.read_parquet(os.path.join(extract.DATA_DIR, \"measurements.parquet\"))\n",
    "df_covered_blocks = pd.read_parquet(os.path.join(extract.DATA_DIR, \"covered_blocks.parquet\"))\n",
    "df_paths = pd.read_parquet(os.path.join(extract.DATA_DIR, \"paths.parquet\"))\n",
    "df_crashes = pd.read_parquet(os.path.join(extract.DATA_DIR, \"crashes.parquet\"))\n",
    "\n",
    "# Get maxes in steps\n",
    "def get_max(steps=1):\n",
    "    dfs = {}\n",
    "    for i in range(1, steps + 1):\n",
    "        trial_time = (experiment.TRIAL_TIME / steps) * i\n",
    "        df_lim = df_measurements.loc[df_measurements[\"cpu_time\"] <= trial_time]\n",
    "        df_lim = df_lim.loc[\n",
    "            df_lim.groupby([\"experiment\", \"benchmark\", \"trial\"])[\"cpu_time\"].idxmax()\n",
    "        ]\n",
    "        dfs[trial_time] = df_lim\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = \"crashes_total\"\n",
    "\n",
    "for trial_time, df_max in get_max(10).items():\n",
    "\n",
    "    # Initialization\n",
    "    wins = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        wins[exp] = 0\n",
    "\n",
    "    # Calculate for each benchmark\n",
    "    for benchmark in experiment.BENCHMARKS:\n",
    "        # print(f\"\\n\\n{benchmark}\")\n",
    "        df_benchmark = df_max.loc[df_max[\"benchmark\"] == benchmark]\n",
    "        for exp1, exp2 in itertools.combinations(extract.EXPERIMENTS, 2):\n",
    "            x = df_benchmark.loc[df_benchmark[\"experiment\"] == exp1][y_key]\n",
    "            y = df_benchmark.loc[df_benchmark[\"experiment\"] == exp2][y_key]\n",
    "            # print(f\"{exp1} - {x.mean()}, {exp2} - {y.mean()}\")\n",
    "            # print(mannwhitneyu(x, y))\n",
    "            try:\n",
    "                _, p = mannwhitneyu(x, y)\n",
    "            except:\n",
    "                continue\n",
    "            if p < 0.05:\n",
    "                if x.mean() > y.mean():\n",
    "                    # print(f\"{exp1} {x.mean()} over {exp2} {y.mean()}\")\n",
    "                    wins[exp1] += 1\n",
    "                else:\n",
    "                    # print(f\"{exp2} {y.mean()} over {exp1} {x.mean()}\")\n",
    "                    wins[exp2] += 1\n",
    "\n",
    "    # Print result\n",
    "    print(trial_time // 60)\n",
    "    print(dict(sorted(wins.items(), key=lambda item: item[1], reverse=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = \"paths\"\n",
    "for benchmark in experiment.BENCHMARKS:\n",
    "    df_benchmark = df_measurements.loc[(df_measurements[\"benchmark\"] == benchmark)]\n",
    "    plt.figure()\n",
    "    axes = sns.lineplot(\n",
    "        y=y_key,\n",
    "        x=\"cpu_time_round\",\n",
    "        hue=\"experiment\",\n",
    "        data=df_benchmark,\n",
    "        estimator=np.median,\n",
    "        ci=95,\n",
    "    )\n",
    "    # axes.set_xscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = \"crashes_unique\"\n",
    "coverage_dict = {\"cpu_time\": [], \"experiment\": [], \"score\": []}\n",
    "rank_dict = {\"cpu_time\": [], \"experiment\": [], \"rank\": []}\n",
    "\n",
    "for trial_time, df_max in get_max(20).items():\n",
    "\n",
    "    # Initialization\n",
    "    sum_coverage = {}\n",
    "    sum_ranks = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        sum_coverage[exp] = 0\n",
    "        sum_ranks[exp] = 0\n",
    "\n",
    "    # Get median for each benchmark\n",
    "    for benchmark in experiment.BENCHMARKS:\n",
    "        exp_coverage = {}\n",
    "        df_benchmark = df_max[df_max[\"benchmark\"] == benchmark]\n",
    "        max_coverage = df_benchmark[y_key].max()\n",
    "        for exp in extract.EXPERIMENTS:\n",
    "            median_coverage = df_benchmark.loc[df_benchmark[\"experiment\"] == exp][\n",
    "                y_key\n",
    "            ].median()\n",
    "            sum_coverage[exp] += (\n",
    "                0 if max_coverage == 0 else (median_coverage / max_coverage) * 100.0\n",
    "            )\n",
    "            exp_coverage[exp] = median_coverage\n",
    "        for rank, key in enumerate(\n",
    "            sorted(exp_coverage, key=exp_coverage.get, reverse=True), 1\n",
    "        ):\n",
    "            sum_ranks[key] += rank\n",
    "\n",
    "    # Calculate the scores\n",
    "    coverage_score = {}\n",
    "    rank_score = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        coverage_score[exp] = sum_coverage[exp] / len(experiment.BENCHMARKS)\n",
    "        rank_score[exp] = sum_ranks[exp] / len(experiment.BENCHMARKS)\n",
    "\n",
    "    # Add to dict\n",
    "    for exp, score in coverage_score.items():\n",
    "        coverage_dict[\"cpu_time\"].append(trial_time)\n",
    "        coverage_dict[\"experiment\"].append(exp)\n",
    "        coverage_dict[\"score\"].append(score)\n",
    "    for exp, rank in rank_score.items():\n",
    "        rank_dict[\"cpu_time\"].append(trial_time)\n",
    "        rank_dict[\"experiment\"].append(exp)\n",
    "        rank_dict[\"rank\"].append(rank)\n",
    "\n",
    "    # Print results\n",
    "    print(trial_time // 60)\n",
    "    print(dict(sorted(coverage_score.items(), key=lambda item: item[1], reverse=True)))\n",
    "    print(dict(sorted(rank_score.items(), key=lambda item: item[1])))\n",
    "\n",
    "# Line plot\n",
    "df_coverage = pd.DataFrame(coverage_dict)\n",
    "plt.figure()\n",
    "sns.lineplot(y=\"score\", x=\"cpu_time\", hue=\"experiment\", data=df_coverage)\n",
    "df_rank = pd.DataFrame(rank_dict)\n",
    "plt.figure()\n",
    "sns.lineplot(y=\"rank\", x=\"cpu_time\", hue=\"experiment\", data=df_rank)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
