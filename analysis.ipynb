{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "import experiment\n",
    "import extract\n",
    "\n",
    "# Plotting setup\n",
    "sns.set_style(\"whitegrid\", {\"font.family\": \"Arial\"})\n",
    "\n",
    "# Constants\n",
    "PALETTE = {\n",
    "    \"Constant / FLUFFI\": \"#6f4e7b\",\n",
    "    \"FAST / FLUFFI\": \"#c9472f\",\n",
    "    \"Constant / Round-Robin\": \"#ffa056\",\n",
    "    \"FAST / Round-Robin\": \"#f7c860\",\n",
    "    \"Constant / AFLFast\": \"#9dd766\",\n",
    "    \"FAST / AFLFast\": \"#267895\",\n",
    "}  # 8dddd0\n",
    "Y_KEY_LABELS = {\n",
    "    \"paths\": \"# Paths Covered\",\n",
    "    \"covered_blocks\": \"# Blocks Covered\",\n",
    "    \"crashes_unique\": \"# Crashes Found\",\n",
    "}\n",
    "\n",
    "# Load the data\n",
    "df_measurements = pd.read_parquet(\n",
    "    os.path.join(extract.DATA_DIR, \"measurements.parquet\")\n",
    ")\n",
    "df_measurements[\"cpu_seconds_round\"] = df_measurements[\"cpu_time\"].round(-3)\n",
    "df_measurements[\"cpu_hours_round\"] = df_measurements[\"cpu_seconds_round\"] / 3600\n",
    "df_covered_blocks = pd.read_parquet(\n",
    "    os.path.join(extract.DATA_DIR, \"covered_blocks.parquet\")\n",
    ")\n",
    "df_paths = pd.read_parquet(os.path.join(extract.DATA_DIR, \"paths.parquet\"))\n",
    "df_crashes = pd.read_parquet(os.path.join(extract.DATA_DIR, \"crashes.parquet\"))\n",
    "\n",
    "# Get maxes in steps\n",
    "def get_max(steps=1):\n",
    "    dfs = {}\n",
    "    for i in range(1, steps + 1):\n",
    "        trial_time = (experiment.TRIAL_TIME / steps) * i\n",
    "        df_lim = df_measurements.loc[df_measurements[\"cpu_time\"] <= trial_time]\n",
    "        df_lim = df_lim.loc[\n",
    "            df_lim.groupby([\"experiment\", \"benchmark\", \"trial\"])[\"cpu_time\"].idxmax()\n",
    "        ]\n",
    "        dfs[trial_time] = df_lim\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = \"crashes_total\"\n",
    "\n",
    "for trial_time, df_max in get_max(10).items():\n",
    "\n",
    "    # Initialization\n",
    "    wins = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        wins[exp] = 0\n",
    "\n",
    "    # Calculate for each benchmark\n",
    "    for benchmark in experiment.BENCHMARKS:\n",
    "        # print(f\"\\n\\n{benchmark}\")\n",
    "        df_benchmark = df_max.loc[df_max[\"benchmark\"] == benchmark]\n",
    "        for exp1, exp2 in itertools.combinations(extract.EXPERIMENTS, 2):\n",
    "            x = df_benchmark.loc[df_benchmark[\"experiment\"] == exp1][y_key]\n",
    "            y = df_benchmark.loc[df_benchmark[\"experiment\"] == exp2][y_key]\n",
    "            # print(f\"{exp1} - {x.mean()}, {exp2} - {y.mean()}\")\n",
    "            # print(mannwhitneyu(x, y))\n",
    "            try:\n",
    "                _, p = mannwhitneyu(x, y)\n",
    "            except:\n",
    "                continue\n",
    "            if p < 0.05:\n",
    "                if x.mean() > y.mean():\n",
    "                    # print(f\"{exp1} {x.mean()} over {exp2} {y.mean()}\")\n",
    "                    wins[exp1] += 1\n",
    "                else:\n",
    "                    # print(f\"{exp2} {y.mean()} over {exp1} {x.mean()}\")\n",
    "                    wins[exp2] += 1\n",
    "\n",
    "    # Print result\n",
    "    print(trial_time // 60)\n",
    "    print(dict(sorted(wins.items(), key=lambda item: item[1], reverse=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = \"covered_blocks\"\n",
    "for benchmark in experiment.BENCHMARKS:\n",
    "    df_benchmark = df_measurements.loc[(df_measurements[\"benchmark\"] == benchmark)]\n",
    "    plt.figure(figsize=(6, 4), dpi=100)\n",
    "    g = sns.lineplot(\n",
    "        y=y_key,\n",
    "        x=\"cpu_hours_round\",\n",
    "        hue=\"experiment\",\n",
    "        palette=PALETTE,\n",
    "        data=df_benchmark,\n",
    "        estimator=np.median,\n",
    "    )\n",
    "    g.legend(title=None)\n",
    "    g.set_xlim(0, 30)\n",
    "    g.set_xlabel(\"CPU Hours\")\n",
    "    g.set_ylabel(Y_KEY_LABELS[y_key])\n",
    "    g.set_title(benchmark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = \"paths\"\n",
    "coverage_dict = {\"cpu_hours\": [], \"experiment\": [], \"score\": []}\n",
    "rank_dict = {\"cpu_hours\": [], \"experiment\": [], \"rank\": []}\n",
    "\n",
    "for trial_time, df_max in get_max(100).items():\n",
    "\n",
    "    # Initialization\n",
    "    sum_coverage = {}\n",
    "    sum_ranks = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        sum_coverage[exp] = 0\n",
    "        sum_ranks[exp] = 0\n",
    "\n",
    "    # Get median for each benchmark\n",
    "    for benchmark in experiment.BENCHMARKS:\n",
    "        exp_coverage = {}\n",
    "        df_benchmark = df_max[df_max[\"benchmark\"] == benchmark]\n",
    "        max_coverage = df_benchmark[y_key].max()\n",
    "        for exp in extract.EXPERIMENTS:\n",
    "            median_coverage = df_benchmark.loc[df_benchmark[\"experiment\"] == exp][\n",
    "                y_key\n",
    "            ].median()\n",
    "            sum_coverage[exp] += (\n",
    "                0 if max_coverage == 0 else (median_coverage / max_coverage) * 100.0\n",
    "            )\n",
    "            exp_coverage[exp] = median_coverage\n",
    "        for rank, key in enumerate(\n",
    "            sorted(exp_coverage, key=exp_coverage.get, reverse=True), 1\n",
    "        ):\n",
    "            sum_ranks[key] += rank\n",
    "\n",
    "    # Calculate the scores\n",
    "    coverage_score = {}\n",
    "    rank_score = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        coverage_score[exp] = sum_coverage[exp] / len(experiment.BENCHMARKS)\n",
    "        rank_score[exp] = sum_ranks[exp] / len(experiment.BENCHMARKS)\n",
    "\n",
    "    # Add to dict\n",
    "    for exp, score in coverage_score.items():\n",
    "        coverage_dict[\"cpu_hours\"].append(trial_time / 3600)\n",
    "        coverage_dict[\"experiment\"].append(exp)\n",
    "        coverage_dict[\"score\"].append(score)\n",
    "    for exp, rank in rank_score.items():\n",
    "        rank_dict[\"cpu_hours\"].append(trial_time / 3600)\n",
    "        rank_dict[\"experiment\"].append(exp)\n",
    "        rank_dict[\"rank\"].append(rank)\n",
    "\n",
    "    # Sort and print results\n",
    "    if trial_time == experiment.TRIAL_TIME:\n",
    "        coverage_score_sorted = dict(\n",
    "            sorted(coverage_score.items(), key=lambda item: item[1], reverse=True)\n",
    "        )\n",
    "        rank_score_sorted = dict(\n",
    "            sorted(rank_score.items(), key=lambda item: item[1], reverse=True)\n",
    "        )\n",
    "        print(coverage_score_sorted)\n",
    "        print(rank_score_sorted)\n",
    "\n",
    "# Coverage line plot\n",
    "df_coverage = pd.DataFrame(coverage_dict)\n",
    "plt.figure(figsize=(6, 4), dpi=100)\n",
    "g = sns.lineplot(\n",
    "    y=\"score\",\n",
    "    x=\"cpu_hours\",\n",
    "    hue=\"experiment\",\n",
    "    hue_order=coverage_score_sorted.keys(),\n",
    "    palette=PALETTE,\n",
    "    data=df_coverage,\n",
    ")\n",
    "g.legend(title=None)\n",
    "g.set_xlim(0, 30)\n",
    "g.set_xlabel(\"CPU Hours\")\n",
    "g.set_ylabel(\"Average Normalized Score\")\n",
    "\n",
    "# Rank line plot\n",
    "df_rank = pd.DataFrame(rank_dict)\n",
    "plt.figure(figsize=(6, 4), dpi=100)\n",
    "g = sns.lineplot(\n",
    "    y=\"rank\",\n",
    "    x=\"cpu_hours\",\n",
    "    hue=\"experiment\",\n",
    "    hue_order=rank_score_sorted.keys(),\n",
    "    palette=PALETTE,\n",
    "    data=df_rank,\n",
    ")\n",
    "g.legend(title=None)\n",
    "g.set_xlim(0, 30)\n",
    "g.set_xlabel(\"CPU Hours\")\n",
    "g.set_ylabel(\"Average Rank\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
