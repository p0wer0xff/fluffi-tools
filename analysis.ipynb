{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "import experiment\n",
    "import extract\n",
    "\n",
    "# Plotting setup\n",
    "sns.set_style(\"whitegrid\", {\"font.family\": \"Arial\"})\n",
    "\n",
    "# Constants\n",
    "PALETTE = {\n",
    "    \"Constant / FLUFFI\": \"#6f4e7b\",\n",
    "    \"FAST / FLUFFI\": \"#c9472f\",\n",
    "    \"Constant / Round-Robin\": \"#ffa056\",\n",
    "    \"FAST / Round-Robin\": \"#f7c860\",\n",
    "    \"Constant / AFLFast\": \"#9dd766\",\n",
    "    \"FAST / AFLFast\": \"#267895\",\n",
    "}  # 8dddd0\n",
    "Y_KEY_LABELS = {\n",
    "    \"paths\": \"# Paths Covered\",\n",
    "    \"covered_blocks\": \"# Blocks Covered\",\n",
    "    \"crashes_unique\": \"# Crashes Found\",\n",
    "}\n",
    "\n",
    "# Load the data\n",
    "df_measurements = pd.read_parquet(\"measurements.parquet\")\n",
    "df_measurements[\"cpu_seconds_round\"] = df_measurements[\"cpu_time\"].round(-3)\n",
    "df_measurements[\"cpu_hours_round\"] = df_measurements[\"cpu_seconds_round\"] / 3600\n",
    "df_measurements[\"bugs\"] = (\n",
    "    df_measurements[\"crashes_unique\"] + df_measurements[\"access_violations_unique\"]\n",
    ")\n",
    "df_measurements[\"covered_blocks_exec\"] = (\n",
    "    df_measurements[\"covered_blocks\"] / df_measurements[\"completed_testcases\"]\n",
    ")\n",
    "df_measurements[\"paths_exec\"] = (\n",
    "    df_measurements[\"paths\"] / df_measurements[\"completed_testcases\"]\n",
    ")\n",
    "\n",
    "# Get maxes in steps\n",
    "def get_max(steps=1):\n",
    "    dfs = {}\n",
    "    for i in range(1, steps + 1):\n",
    "        trial_time = (experiment.TRIAL_TIME / steps) * i\n",
    "        df_lim = df_measurements.loc[df_measurements[\"cpu_time\"] <= trial_time]\n",
    "        df_lim = df_lim.loc[\n",
    "            df_lim.groupby([\"experiment\", \"benchmark\", \"trial\"])[\"cpu_time\"].idxmax()\n",
    "        ]\n",
    "        dfs[trial_time] = df_lim\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Whitney U test\n",
    "\n",
    "y_key = \"covered_blocks\"\n",
    "\n",
    "for trial_time, df_max in get_max(30).items():\n",
    "    print(trial_time / 3600)\n",
    "    wins = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        wins[exp] = 0\n",
    "\n",
    "    # For each experiment combo\n",
    "    for exp_x, exp_y in itertools.combinations(extract.EXPERIMENTS, 2):\n",
    "        result = {exp_x: 0, exp_y: 0, \"Inconclusive\": 0}\n",
    "        result_b = {exp_x: [], exp_y: [], \"Inconclusive\": []}\n",
    "\n",
    "        # Calculate for each benchmark\n",
    "        for benchmark in experiment.BENCHMARKS:\n",
    "            df_benchmark = df_max.loc[df_max[\"benchmark\"] == benchmark]\n",
    "            if df_benchmark[y_key].max() == 0:\n",
    "                continue\n",
    "            x = df_benchmark.loc[df_benchmark[\"experiment\"] == exp_x][y_key]\n",
    "            y = df_benchmark.loc[df_benchmark[\"experiment\"] == exp_y][y_key]\n",
    "            try:\n",
    "                _, p = mannwhitneyu(x, y)\n",
    "            except:\n",
    "                result[\"Inconclusive\"] += 1\n",
    "                result_b[\"Inconclusive\"].append(benchmark)\n",
    "                continue\n",
    "            if p < 0.05:\n",
    "                if x.mean() > y.mean():\n",
    "                    result[exp_x] += 1\n",
    "                    result_b[exp_x].append(benchmark)\n",
    "                    wins[exp_x] += 1\n",
    "                else:\n",
    "                    result[exp_y] += 1\n",
    "                    result_b[exp_y].append(benchmark)\n",
    "                    wins[exp_y] += 1\n",
    "            else:\n",
    "                result[\"Inconclusive\"] += 1\n",
    "                result_b[\"Inconclusive\"].append(benchmark)\n",
    "\n",
    "        # Print result\n",
    "        print(result)\n",
    "        print(result_b)\n",
    "    print(sorted(wins.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line graphs for code coverage over time for a single benchmark\n",
    "\n",
    "hue_orders = {\n",
    "    \"covered_blocks\": [\n",
    "        \"FAST / AFLFast\",\n",
    "        \"Constant / FLUFFI\",\n",
    "        \"Constant / AFLFast\",\n",
    "        \"FAST / FLUFFI\",\n",
    "        \"FAST / Round-Robin\",\n",
    "        \"Constant / Round-Robin\",\n",
    "    ],\n",
    "    \"paths\": [\n",
    "        \"FAST / AFLFast\",\n",
    "        \"Constant / AFLFast\",\n",
    "        \"Constant / FLUFFI\",\n",
    "        \"FAST / FLUFFI\",\n",
    "        \"FAST / Round-Robin\",\n",
    "        \"Constant / Round-Robin\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for y_key in [\"covered_blocks\", \"paths\"]:\n",
    "    for benchmark in experiment.BENCHMARKS:\n",
    "        if \"njs\" not in benchmark:\n",
    "            continue\n",
    "        df_benchmark = df_measurements.loc[(df_measurements[\"benchmark\"] == benchmark)]\n",
    "        plt.figure(figsize=(6, 4), dpi=100)\n",
    "        g = sns.lineplot(\n",
    "            y=y_key,\n",
    "            x=\"cpu_hours_round\",\n",
    "            hue=\"experiment\",\n",
    "            hue_order=hue_orders[y_key],\n",
    "            palette=PALETTE,\n",
    "            data=df_benchmark,\n",
    "            estimator=np.median,\n",
    "            ci=95,\n",
    "        )\n",
    "        g.legend(title=None)\n",
    "        g.set_xlim(0, 30)\n",
    "        if y_key == \"covered_blocks\":\n",
    "            g.set_ylim(2000)\n",
    "        elif y_key == \"paths\":\n",
    "            g.set_ylim(0)\n",
    "        g.set_xlabel(\"CPU Hours\")\n",
    "        g.set_ylabel(Y_KEY_LABELS[y_key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs for average normalized score\n",
    "\n",
    "y_key = \"paths_exec\"\n",
    "coverage_dict = {\"cpu_hours\": [], \"experiment\": [], \"score\": []}\n",
    "rank_dict = {\"cpu_hours\": [], \"experiment\": [], \"rank\": []}\n",
    "\n",
    "for trial_time, df_max in get_max(180).items():\n",
    "\n",
    "    # Initialization\n",
    "    sum_coverage = {}\n",
    "    sum_ranks = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        sum_coverage[exp] = 0\n",
    "        sum_ranks[exp] = 0\n",
    "\n",
    "    # Get median for each benchmark\n",
    "    for benchmark in experiment.BENCHMARKS:\n",
    "        df_benchmark = df_max[df_max[\"benchmark\"] == benchmark]\n",
    "        if df_benchmark[y_key].max() == 0:\n",
    "            continue\n",
    "        exp_coverage = {}\n",
    "        max_coverage = df_benchmark[y_key].max()\n",
    "        for exp in extract.EXPERIMENTS:\n",
    "            median_coverage = df_benchmark.loc[df_benchmark[\"experiment\"] == exp][\n",
    "                y_key\n",
    "            ].median()\n",
    "            sum_coverage[exp] += (\n",
    "                0 if max_coverage == 0 else (median_coverage / max_coverage) * 100.0\n",
    "            )\n",
    "            exp_coverage[exp] = median_coverage\n",
    "        for rank, key in enumerate(\n",
    "            sorted(exp_coverage, key=exp_coverage.get, reverse=True), 1\n",
    "        ):\n",
    "            sum_ranks[key] += rank\n",
    "\n",
    "    # Calculate the scores\n",
    "    coverage_score = {}\n",
    "    rank_score = {}\n",
    "    for exp in extract.EXPERIMENTS:\n",
    "        coverage_score[exp] = sum_coverage[exp] / len(experiment.BENCHMARKS)\n",
    "        rank_score[exp] = sum_ranks[exp] / len(experiment.BENCHMARKS)\n",
    "\n",
    "    # Add to dict\n",
    "    for exp, score in coverage_score.items():\n",
    "        coverage_dict[\"cpu_hours\"].append(trial_time / 3600)\n",
    "        coverage_dict[\"experiment\"].append(exp)\n",
    "        coverage_dict[\"score\"].append(score)\n",
    "    for exp, rank in rank_score.items():\n",
    "        rank_dict[\"cpu_hours\"].append(trial_time / 3600)\n",
    "        rank_dict[\"experiment\"].append(exp)\n",
    "        rank_dict[\"rank\"].append(rank)\n",
    "\n",
    "    # Sort and print results\n",
    "    if trial_time == experiment.TRIAL_TIME:\n",
    "        coverage_score_sorted = dict(\n",
    "            sorted(coverage_score.items(), key=lambda item: item[1], reverse=True)\n",
    "        )\n",
    "        rank_score_sorted = dict(\n",
    "            sorted(rank_score.items(), key=lambda item: item[1], reverse=True)\n",
    "        )\n",
    "        print(coverage_score_sorted)\n",
    "        print(rank_score_sorted)\n",
    "\n",
    "# Coverage bar plot\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"whitegrid\", {\"font.family\": \"Arial\"})\n",
    "plt.figure(figsize=(6, 4), dpi=100)\n",
    "g = sns.barplot(\n",
    "    y=list(coverage_score_sorted.keys()),\n",
    "    x=list(coverage_score_sorted.values()),\n",
    "    palette=PALETTE,\n",
    ")\n",
    "if y_key == \"paths\":\n",
    "    g.set_xlim(70, 80)\n",
    "elif y_key == \"paths_exec\":\n",
    "    g.set_xlim(72, 82)\n",
    "elif y_key == \"covered_blocks\":\n",
    "    g.set_xlim(90, 96)\n",
    "elif y_key == \"covered_blocks_exec\":\n",
    "    g.set_xlim(80, 86)\n",
    "elif y_key == \"bugs\":\n",
    "    g.set_xlim(30, 34)\n",
    "elif y_key == \"completed_testcases\":\n",
    "    g.set_xlim(90, 92.5)\n",
    "g.set_xlabel(\"Average Normalized Score\")\n",
    "\n",
    "# Rank bar plot\n",
    "plt.figure(figsize=(6, 6), dpi=100)\n",
    "rank_score_sorted_y = list(rank_score_sorted.keys())\n",
    "rank_score_sorted_y.reverse()\n",
    "rank_score_sorted_x = list(rank_score_sorted.values())\n",
    "rank_score_sorted_x.reverse()\n",
    "g = sns.barplot(\n",
    "    y=rank_score_sorted_y,\n",
    "    x=rank_score_sorted_x,\n",
    "    palette=PALETTE,\n",
    ")\n",
    "g.set_xlim(1, 5)\n",
    "g.set_xlabel(\"Average Rank\")\n",
    "\n",
    "# Coverage line plot\n",
    "df_coverage = pd.DataFrame(coverage_dict)\n",
    "plt.figure(figsize=(6, 4), dpi=100)\n",
    "g = sns.lineplot(\n",
    "    y=\"score\",\n",
    "    x=\"cpu_hours\",\n",
    "    hue=\"experiment\",\n",
    "    hue_order=coverage_score_sorted.keys(),\n",
    "    palette=PALETTE,\n",
    "    data=df_coverage,\n",
    ")\n",
    "g.legend(title=None)\n",
    "g.set_xlim(0, 30)\n",
    "g.set_xlabel(\"CPU Hours\")\n",
    "g.set_ylabel(\"Average Normalized Score\")\n",
    "\n",
    "# Rank line plot\n",
    "df_rank = pd.DataFrame(rank_dict)\n",
    "plt.figure(figsize=(6, 4), dpi=100)\n",
    "g = sns.lineplot(\n",
    "    y=\"rank\",\n",
    "    x=\"cpu_hours\",\n",
    "    hue=\"experiment\",\n",
    "    hue_order=rank_score_sorted.keys(),\n",
    "    palette=PALETTE,\n",
    "    data=df_rank,\n",
    ")\n",
    "g.legend(title=None)\n",
    "g.set_xlim(0, 30)\n",
    "g.set_xlabel(\"CPU Hours\")\n",
    "g.set_ylabel(\"Average Rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTeX table describing each benchmark\n",
    "\n",
    "benchmarks_sorted = sorted(experiment.BENCHMARKS)\n",
    "for benchmark in benchmarks_sorted:\n",
    "    benchmark_dir = os.path.join(experiment.FUZZBENCH_DIR, benchmark)\n",
    "    seeds_path = os.path.join(benchmark_dir, \"seeds/\")\n",
    "    seeds = []\n",
    "    for seed in os.listdir(seeds_path):\n",
    "        seed_path = os.path.join(seeds_path, seed)\n",
    "        with open(seed_path, \"rb\") as f:\n",
    "            data = f.read()\n",
    "        seeds.append((seed, data))\n",
    "    with open(os.path.join(benchmark_dir, \"target.txt\"), \"r\") as f:\n",
    "        target_name = f.read().strip()\n",
    "    target_path = os.path.join(benchmark_dir, target_name)\n",
    "    num = os.path.getsize(target_path)\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\"]:\n",
    "        if abs(num) < 1024.0:\n",
    "            size = f\"{num:3.1f} {unit}B\"\n",
    "            break\n",
    "        num /= 1024.0\n",
    "    print(\n",
    "        f\"{benchmark} & ??? & {min(len(seeds), experiment.SEED_NUM_LIMIT)} & {size} \\\\\\\\ \\\\hline\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTeX table for bugs found\n",
    "\n",
    "for benchmark in sorted(experiment.BENCHMARKS):\n",
    "    df_benchmark = df_measurements.loc[(df_measurements[\"benchmark\"] == benchmark)]\n",
    "    val = df_benchmark[\"bugs\"].max()\n",
    "    benchmark = benchmark.replace(\"_\", r\"{\\_}\")\n",
    "    print(rf\"{benchmark} & {val} \\\\ \\hline\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
